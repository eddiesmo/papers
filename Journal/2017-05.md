2015-05-25:
1.  Image Segmentation by Iterative Inference from Conditional Score Estimation

    https://arxiv.org/abs/1705.07450

    A net that performs slightly better than CRF for image segmentation fine-tuning/post-processing.

    Questions:
    1. What’s the runtime, compared with CRF?
    2. Will the code/model be publicly available?

2.  Explored https://github.com/oarriaga/face_classification

    `Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV.`

    very simple example for KERAS use

3.  Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics

    https://arxiv.org/pdf/1705.07115.pdf

    Abstract:
    > Numerous deep learning applications benefit from multi-task learning with multiple
    regression and classification objectives. In this paper we make the observation that
    the performance of such systems is strongly dependent on the relative weighting
    between each task’s loss. Tuning these weights by hand is a difficult and expensive
    process, making multi-task learning prohibitive in practice. We propose a
    principled approach to multi-task deep learning which weighs multiple loss functions
    by considering the homoscedastic uncertainty of each task. This allows us
    to simultaneously learn various quantities with different units or scales in both
    classification and regression settings. We demonstrate our model learning per-pixel
    depth regression, semantic and instance segmentation from a monocular input
    image. Perhaps surprisingly, we show our model can learn multi-task weightings
    and outperform separate models trained individually on each task.

    Doesn't look like a serious paper. Barely tested on any segmentation datasets and the IoU results seem bad to me.

4.  The Kinetics Human Action Video Dataset (from Google)

    https://arxiv.org/pdf/1705.06950.pdf

    Abstract:
    > We describe the DeepMind Kinetics human action video
    dataset. The dataset contains 400 human action classes,
    with at least 400 video clips for each action. Each clip lasts
    around 10s and is taken from a different YouTube video. The
    actions are human focussed and cover a broad range of
    classes including human-object interactions such as playing
    instruments, as well as human-human interactions such
    as shaking hands. We describe the statistics of the dataset,
    how it was collected, and give some baseline performance
    figures for neural network architectures trained and tested
    for human action classification on this dataset. We also
    carry out a preliminary analysis of whether imbalance in
    the dataset leads to bias in the classifiers.